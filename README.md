# RecallAI

**Intelligent local file search** - Search your documents and code using natural language, completely offline.

## Features

- ğŸ” **Semantic Search** - Find files by meaning, not just keywords
- ğŸ¤– **LLM Answers** - Get natural language answers from your content
- ğŸ’¾ **Fully Local** - No cloud, no tracking, complete privacy
- âš¡ **Fast** - Pre-indexed embeddings for instant results
- ğŸ“ **Multiple Formats** - `.txt`, `.md`, `.pdf`, `.py`, `.ipynb`

## Quick Start

### 1. Setup (One Time)

```bash
./setup.sh
```

This installs everything: `uv`, Ollama, Llama 3.1 model, and dependencies.

### 2. Run

```bash
source .venv/bin/activate
python recall_ai/app.py
```

### 3. Use

Open http://localhost:8000 in your browser.

1. **Configure Folders** - Add paths to index
2. **Index Files** - Click "Start Indexing"
3. **Search** - Ask questions in natural language

## Search Modes

- **Search Mode** - Returns matching files and chunks
- **Answer Mode** - Uses LLM to generate answers with sources

## Requirements

- Python 3.11+
- ~8GB RAM
- macOS or Linux

## Documentation

- [Quick Start Guide](QUICKSTART.md) - Detailed setup
- [API Documentation](docs/API.md) - REST API reference
- [Requirements](REQUIREMENTS.md) - Full project spec
- [Design Docs](docs/desings.md) - Architecture details

## Technology

- **Backend**: FastAPI + Gradio UI
- **Search**: FAISS (semantic vector search)
- **Embeddings**: Sentence Transformers
- **LLM**: Llama 3.1 8B via Ollama
- **Package Manager**: uv (fast Python installs)

## Makefile Commands

```bash
make setup   # One-time setup
make run     # Start RecallAI
make clean   # Clear indexes/models
make help    # Show all commands
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RecallAI (Single Process)         â”‚
â”‚                                    â”‚
â”‚  FastAPI â”€â”€â”€â”€â”¬â”€â”€â”€â”€ Gradio UI       â”‚
â”‚              â”‚                     â”‚
â”‚  Search â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚  Indexing â”€â”€â”€â”¤                     â”‚
â”‚  Embeddings â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
        Ollama (LLM)
     localhost:11434
```

Everything runs locally. No data leaves your machine.

## Design Principles

- **Offline-first** - No cloud dependencies
- **Pre-indexed** - Embeddings computed upfront for speed
- **Incremental** - Only re-index changed files
- **Simple** - Clean API, no over-engineering

## Project Structure

```
recall_ai/
â”œâ”€â”€ app.py              # Main entry point
â”œâ”€â”€ gateway/            # FastAPI server & Gradio UI
â”œâ”€â”€ processing/         # Search & LLM integration
â”œâ”€â”€ embeddings/         # Vector search (FAISS)
â”œâ”€â”€ parsers/            # File parsing & chunking
â””â”€â”€ utils/              # Config, logging, scanning
```

## License

MIT

## Contributing

This is a learning project. Feel free to fork and experiment!
